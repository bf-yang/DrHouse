# Two LLM agents simluation consultation
# DrHouse vs simulated patient
# Local Qwen as base LLM
import os
import sys
sys.path.append('/home/bufang/DrHouse')
from openai import AzureOpenAI
from langchain.prompts import ChatPromptTemplate
from src.utils import get_dialogue_demos,get_system_prompt,get_medical_knowledge,\
    get_guideline_trees,adaptive_retrieval,get_sensordata_knowledge
import argparse
import torch
from transformers import BertTokenizer, BertTokenizer
from src.semantic_filter import BertClassifier
import os
from pprint import pprint
import warnings
warnings.filterwarnings('ignore')
import transformers
from transformers import logging
logging.set_verbosity_error()
from transformers import AutoModelForCausalLM, AutoTokenizer


api_key = "4d2ff10a8c3d4d09883a4411832b6718"

model_id_dict = {
    'qwen1.5b':"Qwen/Qwen2.5-1.5B-Instruct",
    'qwen7b':"Qwen/Qwen2.5-7B-Instruct"
}

# LLM agent patient
client_patient = AzureOpenAI(
  api_key = api_key,  
  api_version = "2023-05-15",
  azure_endpoint = "https://cuhk-aiot-gpt4.openai.azure.com/"
)

# # LLM agent doctor
# client_doctor = AzureOpenAI(
#   api_key = api_key,  
#   api_version = "2023-05-15",
#   azure_endpoint = "https://cuhk-aiot-gpt4.openai.azure.com/"
# )

# LLM agent coordinator
client_coordinator = AzureOpenAI(
  api_key = api_key,  
  api_version = "2023-05-15",
  azure_endpoint = "https://cuhk-aiot-gpt4.openai.azure.com/"
)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_version", type=str, default='gpt-4o',
                        help='gpt-35-turbo, gpt-4o or gpt-4-1106') # simulated patient and coordinator
    parser.add_argument("--model_base", type=str, default='qwen7b',
                        help='qwen1.5b,qwen7b')              # DrHouse's base LLM
    parser.add_argument("--mode_guideline_tree", type=str, default='mapping-based',
                        help='mapping-based: mapping-based approach for retrieval, MedDM: guideline tree vector DB')
    parser.add_argument("--exp",type=str,default='simulation', help='real or simulation')
    parser.add_argument("--user_name",type=str,default='sim_data_ab_1')

    args = parser.parse_args()

    base_model_id = model_id_dict[args.model_base]
    print("DrHouse Base LLM: ", base_model_id)

    # Set the maximum number of tokens generated by the LLM
    max_new_tokens = 4096

    # Set patient agent
    symptoms = 'I have a fever, yellow sputum, chilled, sore throat, sore throat, muscle aches, negative COVID-19.'
    # Read fixed prompt
    file_path = 'src/simulation/prompt/patient_simulation/prompt_patient.txt'
    with open(file_path, 'r') as file:
        prompt_patient = file.read()
    # Add symptoms to prompt 
    prompt_patient = ChatPromptTemplate.from_template(prompt_patient)
    prompt_patient = prompt_patient.format(symptoms=symptoms) 
    conversation_patient=[{"role": "system", "content": prompt_patient}]

    # Set coordinator agent
    file_path = 'src/simulation/prompt/patient_simulation/prompt_coordinator.txt'
    with open(file_path, 'r') as file:
        prompt_coordinator = file.read()
    conversation_coordinator=[{"role": "system", "content": prompt_coordinator}]


    # Path of the patient's sensor data embedding database.
    if args.exp == 'real':
        path_sensor_db_base = 'exp_real/data/vector_databases'
        user_name = args.user_name  #'cj_t2'
    if args.exp == 'simulation':
        path_sensor_db_base = 'exp_simulation/data/vector_databases'
        user_name = args.user_name  #'sim_data_ab_1'

    path_sensor_db = os.path.join(path_sensor_db_base,user_name)

    # System settings
    device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")

    # Propmt path
    prompt_path = "prompt"
    path_demo = 'data/medical_dialogues/MedDialog/english-train.json'
    path_guideline = 'data/guideline_trees/txt_guidelines' # path of guideline tree txt files
    path_guideline_vdb = 'xxx'  # path of vector DB
    
    # Adaptive retrieval settings
    model_ar = BertClassifier(flag="base")  # ar: adaptive retrieval
    tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')
    save_path = 'checkpoints/semantic_filter.pt'
    model_ar.load_state_dict(torch.load(save_path), strict=False)
    
    # Information fusion template
    TEMPLATE_PATIENT_REPLY = """
    # Patient:
    {patient_response}

    # Knowledge retrieved from medical textbooks
    {knowledge_med}

    # Knowledge retrieved from sensor data
    {knowledge_sensor}
    """

    # Initialize the local LLM (Qwen) of DrHouse
    model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        torch_dtype="auto",
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model_id)
    

    # Start medical consultation
    MAX_TURN = 10
    n_turn = 0         # n-th turn
    cnt_sensor_retrieval = 0 # number of sensor data retrieval
    symptom_total = '' # total symptoms reported by each turn
    sensor_knowledge = None
    dialogues = []     # record dialogues and save
    while n_turn < MAX_TURN:
        # user_input = input("[ Patient Symptoms ]: \n") 
        user_input = client_patient.chat.completions.create(
            model=args.model_version,
            messages=conversation_patient,
            temperature=0.5
        )
        user_input = user_input.choices[0].message.content
        print('\033[95mPatient Symptoms: \n', user_input, '\033[0m')
        # print("[ Patient Symptoms ]: \n", user_input)

        # Dialogue demos retrieval
        if n_turn == 0:
            demo_topk = get_dialogue_demos(path_demo, user_input)
        if user_input == 'end': # End consultation
            break

        symptom_total += user_input # cumulative symptoms

        # Retrieve medical knowledge (retrieved by user current input)
        medical_knowledge = get_medical_knowledge(user_input, sensor_knowledge)
        # Merge medical knoweldge, n-1 turn sensor data with current user input
        patient_reply_template = ChatPromptTemplate.from_template(TEMPLATE_PATIENT_REPLY)
        user_input_merge = patient_reply_template.format(patient_response=user_input, 
                                            knowledge_med=medical_knowledge,
                                            knowledge_sensor=sensor_knowledge)        

        # Retrieve guideline trees (retrieved by user total symptoms)
        print(70*'-')
        guideline_trees,disease_list, simscore_list = get_guideline_trees(path_guideline_vdb, 
                                    path_guideline, symptom_total, args.mode_guideline_tree)
        print(70*'-')

        # LLM Decision-making
        # Update system prompt
        system_prompt = get_system_prompt(guideline_trees,disease_list, simscore_list, prompt_path)
        if n_turn == 0:
            conversation=[{"role": "system", "content": system_prompt}]  # initailize conversation
        else:
            conversation[0]['content'] = system_prompt # update system prompt (update guideline trees)
        conversation.append({"role": "user", "content": user_input_merge})
        # LLM inference
        # # api infence
        # response = client_doctor.chat.completions.create(
        #     model=args.model_version,
        #     messages=conversation,
        #     temperature=0.5
        # )
        # conversation.append({"role": "assistant", "content": response.choices[0].message.content})
        # print('\033[96mDrHouse: \n', response.choices[0].message.content, '\033[0m')
        # print("[ DrHouse ]: \n" + response.choices[0].message.content + "\n")
        
        # local inference
        text = tokenizer.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=True,
        )
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=512,
        )
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

        conversation.append({"role": "assistant", "content": response})
        print('\033[96mDrHouse: \n', response, '\033[0m')

        # Save dialogue
        if not sensor_knowledge:
            dialogues.append("[ Patient ]: " + user_input)
        else:
            dialogues.append("[ Patient ]: " + user_input+ 
                                    " (retrieved knowledge from sensor database: "+sensor_knowledge+")")
        dialogues.append("[ DrHouse ]: " + response)

        # Retrieve sensor data
        # Adaptive Retrieval
        pred = adaptive_retrieval(response, # doctor text
                                  tokenizer_bert,
                                  model_ar)
        if pred.item():
            print("Start Retrieval")
            sensor_knowledge = get_sensordata_knowledge(response, path_sensor_db, summarize=False)
            cnt_sensor_retrieval += 1
        else:
            print("No Sensor Data Retrieval")

        n_turn += 1


        # Add the doctor's question to the conversation context of patient agent
        conversation_patient.append({"role": "assistant", "content": response})

        if n_turn >=3:
            # Judge whether to end the negotiation
            dialogues_string = '\n'.join(dialogues) # dialogues is string list, while LLM input must be string
            conversation_coordinator.append({"role": "user", "content": dialogues_string})
            response_coordinator = client_coordinator.chat.completions.create(
                # model="gpt-35-turbo",
                model="gpt-4o",
                messages=conversation_coordinator, 
                temperature=0.5
            )
            response_coordinator = response_coordinator.choices[0].message.content
            print("If the consultation is over? ",response_coordinator)
            if response_coordinator == 'Yes':
                break

    dialogues.append('\n')
    dialogues.append(["cnt_sensor_retrieval: ", cnt_sensor_retrieval])
    dialogues.append(["n_turn: ", n_turn])

    # Save dialogues
    print("End successful!")
    path_res_save = os.path.join('src/baseLLMs/outputs/dialogues',
                                 'res_'+user_name+'_'+args.model_base+'.txt')
    with open(path_res_save, 'w') as f:
        for line in dialogues:
            f.write(f"{line}\n")